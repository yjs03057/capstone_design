{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/user/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = get_upper_dir()\n",
    "DATA_DIR = get_data_dir(PROJECT_DIR)\n",
    "FEATURE_DIR = get_feature_dir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_labels_np = np.load(FEATURE_DIR + 'gender_age/label2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np = np.load(FEATURE_DIR + 'direction/' + 'label2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(labels_np.reshape(-1, 1))\n",
    "labels = enc.transform(labels_np.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_enc = OneHotEncoder()\n",
    "gender_enc.fit(gender_labels_np.reshape(-1, 1))\n",
    "gender_labels = gender_enc.transform(gender_labels_np.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(FEATURE_DIR + 'direction/feature2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_features = np.load(FEATURE_DIR + 'gender_age/feature2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LeakyReLU, Input, Softmax, Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(xFeed, yFeed):\n",
    "    # 셔플\n",
    "    xFeed_shuf = []\n",
    "    yFeed_shuf = []\n",
    "    index_shuf = list(range(len(xFeed)))\n",
    "    shuffle(index_shuf)\n",
    "    # one more shuffle\n",
    "    shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        xFeed_shuf.append(xFeed[i])\n",
    "        yFeed_shuf.append(yFeed[i])\n",
    "    return xFeed_shuf, yFeed_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_features, gender_labels = shuffle_data(gender_features, gender_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gender_features[:2880]\n",
    "train_label = gender_labels[:2880]\n",
    "test_data = gender_features[2880:]\n",
    "test_label = gender_labels[2880:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = features[:1440]\n",
    "train_label = labels[:1440]\n",
    "test_data = features[1440:]\n",
    "test_label = labels[1440:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape = (120, 150, 1))\n",
    "\n",
    "lrelu = LeakyReLU(alpha=0.1)\n",
    "x = Conv2D(32, kernel_size=5, activation=lrelu)(input)\n",
    "\n",
    "\n",
    "s1 = Conv2D(50, kernel_size=3, activation='relu')(x)\n",
    "s1 = MaxPooling2D(pool_size = (3,3), strides=(2,2))(s1)\n",
    "s2 = Conv2D(50, kernel_size=1, activation='relu')(x)\n",
    "s2 = MaxPooling2D(pool_size = (5,5), strides=(2,2))(s2)\n",
    "s3 = Conv2D(50, kernel_size=5, activation='relu')(x)\n",
    "s3 = MaxPooling2D(pool_size = (1,1), strides=(2,2))(s3)\n",
    "s_output = tf.concat([s1, s2, s3], 1)\n",
    "\n",
    "x = Conv2D(32, kernel_size=1, activation=lrelu)(s_output)\n",
    "x = MaxPooling2D(pool_size = (5,5), strides=(2,2))(x)\n",
    "x = Conv2D(32, kernel_size =5, activation=lrelu)(x)\n",
    "x = MaxPooling2D(pool_size = (1,1), strides=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(500)(x)\n",
    "x = lrelu(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(6)(x)\n",
    "y = Softmax()(x)\n",
    "model = Model(inputs=input, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 120, 150, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_data = train_data[:, :, :,np.newaxis,]\n",
    "test_data = np.array(test_data)\n",
    "test_data = test_data[:, :, :,np.newaxis,]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152 samples, validate on 288 samples\n",
      "Epoch 1/70\n",
      "  32/1152 [..............................] - ETA: 1:44"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": " Operation received an exception:Status: 5, message: could not create a view primitive descriptor, in file tensorflow/core/kernels/mkl_slice_op.cc:433\n\t [[node Slice_1 (defined at /Users/user/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_18668]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-5f5c167b4de7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mAbortedError\u001b[0m:  Operation received an exception:Status: 5, message: could not create a view primitive descriptor, in file tensorflow/core/kernels/mkl_slice_op.cc:433\n\t [[node Slice_1 (defined at /Users/user/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_18668]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_label, batch_size=32, epochs=70, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet : \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size = 5, padding=\"same\", input_shape=input_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "        model.add(Conv2D(50, kernel_size = 5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(6))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_data = train_data[:, :, :,np.newaxis,]\n",
    "test_data = np.array(test_data)\n",
    "test_data = test_data[:, :, :,np.newaxis,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet.build(input_shape = (120, 150, 1), classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152 samples, validate on 288 samples\n",
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 76s 66ms/step - loss: 26.3207 - accuracy: 0.2778 - val_loss: 2.6237 - val_accuracy: 0.3715\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 68s 59ms/step - loss: 1.9331 - accuracy: 0.4010 - val_loss: 1.5722 - val_accuracy: 0.3160\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 1.6432 - accuracy: 0.3733 - val_loss: 1.6544 - val_accuracy: 0.3368\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 1.5273 - accuracy: 0.3620 - val_loss: 1.3009 - val_accuracy: 0.3090\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 1.2085 - accuracy: 0.4089 - val_loss: 0.9954 - val_accuracy: 0.5312\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.8874 - accuracy: 0.6172 - val_loss: 0.7518 - val_accuracy: 0.6285\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.7188 - accuracy: 0.6528 - val_loss: 0.6511 - val_accuracy: 0.7014\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.6054 - accuracy: 0.7057 - val_loss: 0.5796 - val_accuracy: 0.6944\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.5268 - accuracy: 0.7483 - val_loss: 0.5370 - val_accuracy: 0.6806\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.4837 - accuracy: 0.7543 - val_loss: 0.5191 - val_accuracy: 0.7153\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.4193 - accuracy: 0.8186 - val_loss: 0.5262 - val_accuracy: 0.7222\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 68s 59ms/step - loss: 0.3981 - accuracy: 0.8064 - val_loss: 0.4746 - val_accuracy: 0.7465\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.3433 - accuracy: 0.8446 - val_loss: 0.5689 - val_accuracy: 0.7292\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.3732 - accuracy: 0.8351 - val_loss: 0.5609 - val_accuracy: 0.7292\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 63s 55ms/step - loss: 0.3778 - accuracy: 0.8299 - val_loss: 0.5052 - val_accuracy: 0.7188\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.3161 - accuracy: 0.8759 - val_loss: 0.5213 - val_accuracy: 0.7361\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.2801 - accuracy: 0.8924 - val_loss: 0.5327 - val_accuracy: 0.7257\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.2342 - accuracy: 0.9097 - val_loss: 0.5138 - val_accuracy: 0.7326\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.2188 - accuracy: 0.9141 - val_loss: 0.6925 - val_accuracy: 0.7535\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.2644 - accuracy: 0.8872 - val_loss: 0.6639 - val_accuracy: 0.7292\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.3089 - accuracy: 0.8646 - val_loss: 0.6406 - val_accuracy: 0.6875\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 63s 54ms/step - loss: 0.2923 - accuracy: 0.8724 - val_loss: 0.5364 - val_accuracy: 0.7153\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.2567 - accuracy: 0.8950 - val_loss: 0.5579 - val_accuracy: 0.7188\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 66s 58ms/step - loss: 0.2031 - accuracy: 0.9280 - val_loss: 0.5855 - val_accuracy: 0.7292\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.1902 - accuracy: 0.9358 - val_loss: 0.6121 - val_accuracy: 0.7222\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.2152 - accuracy: 0.9210 - val_loss: 0.5300 - val_accuracy: 0.7500\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 64s 55ms/step - loss: 0.1698 - accuracy: 0.9340 - val_loss: 0.5289 - val_accuracy: 0.7361\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.1498 - accuracy: 0.9497 - val_loss: 0.5602 - val_accuracy: 0.7431\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.1306 - accuracy: 0.9635 - val_loss: 0.5966 - val_accuracy: 0.7639\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 68s 59ms/step - loss: 0.1147 - accuracy: 0.9679 - val_loss: 0.5639 - val_accuracy: 0.7569\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 69s 60ms/step - loss: 0.1266 - accuracy: 0.9601 - val_loss: 0.5917 - val_accuracy: 0.7569\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 70s 61ms/step - loss: 0.1084 - accuracy: 0.9705 - val_loss: 0.6421 - val_accuracy: 0.7604\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 69s 60ms/step - loss: 0.0929 - accuracy: 0.9835 - val_loss: 0.5928 - val_accuracy: 0.7604\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0932 - accuracy: 0.9688 - val_loss: 0.5696 - val_accuracy: 0.7604\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.0751 - accuracy: 0.9861 - val_loss: 0.6220 - val_accuracy: 0.7431\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 63s 55ms/step - loss: 0.0827 - accuracy: 0.9748 - val_loss: 0.6349 - val_accuracy: 0.7431\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.0816 - accuracy: 0.9774 - val_loss: 0.6071 - val_accuracy: 0.7431\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.0792 - accuracy: 0.9818 - val_loss: 0.6312 - val_accuracy: 0.7500\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 63s 55ms/step - loss: 0.0679 - accuracy: 0.9818 - val_loss: 0.6937 - val_accuracy: 0.7326\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0636 - accuracy: 0.9896 - val_loss: 0.7020 - val_accuracy: 0.7569\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.0721 - accuracy: 0.9792 - val_loss: 0.7188 - val_accuracy: 0.7569\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.1014 - accuracy: 0.9670 - val_loss: 0.6657 - val_accuracy: 0.7569\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.0953 - accuracy: 0.9705 - val_loss: 0.6586 - val_accuracy: 0.7153\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.0810 - accuracy: 0.9783 - val_loss: 0.6399 - val_accuracy: 0.7361\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.0799 - accuracy: 0.9748 - val_loss: 0.7105 - val_accuracy: 0.7326\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0625 - accuracy: 0.9852 - val_loss: 0.7111 - val_accuracy: 0.7014\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0768 - accuracy: 0.9792 - val_loss: 0.6743 - val_accuracy: 0.7188\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0617 - accuracy: 0.9818 - val_loss: 0.7146 - val_accuracy: 0.7188\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0606 - accuracy: 0.9800 - val_loss: 0.6055 - val_accuracy: 0.7396\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0583 - accuracy: 0.9861 - val_loss: 0.8499 - val_accuracy: 0.7465\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 62s 54ms/step - loss: 0.0781 - accuracy: 0.9731 - val_loss: 0.7414 - val_accuracy: 0.7188\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0673 - accuracy: 0.9783 - val_loss: 0.5846 - val_accuracy: 0.7396\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0516 - accuracy: 0.9922 - val_loss: 0.5961 - val_accuracy: 0.7361\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0490 - accuracy: 0.9913 - val_loss: 0.6947 - val_accuracy: 0.7222\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 64s 56ms/step - loss: 0.0409 - accuracy: 0.9887 - val_loss: 0.7155 - val_accuracy: 0.7465\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0299 - accuracy: 0.9948 - val_loss: 0.7342 - val_accuracy: 0.7326\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 65s 57ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.7678 - val_accuracy: 0.7465\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0286 - accuracy: 0.9931 - val_loss: 0.7472 - val_accuracy: 0.7882\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0205 - accuracy: 0.9983 - val_loss: 0.7065 - val_accuracy: 0.7465\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0227 - accuracy: 0.9948 - val_loss: 0.7320 - val_accuracy: 0.7535\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0353 - accuracy: 0.9939 - val_loss: 0.7004 - val_accuracy: 0.7465\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 68s 59ms/step - loss: 0.0285 - accuracy: 0.9931 - val_loss: 0.7706 - val_accuracy: 0.7535\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.0264 - accuracy: 0.9965 - val_loss: 0.7434 - val_accuracy: 0.7500\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.0225 - accuracy: 0.9965 - val_loss: 0.8080 - val_accuracy: 0.7326\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 65s 56ms/step - loss: 0.0184 - accuracy: 0.9983 - val_loss: 0.8014 - val_accuracy: 0.7465\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.0326 - accuracy: 0.9931 - val_loss: 0.7636 - val_accuracy: 0.7396\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 67s 58ms/step - loss: 0.0372 - accuracy: 0.9939 - val_loss: 0.8705 - val_accuracy: 0.7014\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 68s 59ms/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 0.7063 - val_accuracy: 0.7674\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 66s 58ms/step - loss: 0.0363 - accuracy: 0.9922 - val_loss: 0.7220 - val_accuracy: 0.7292\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 67s 59ms/step - loss: 0.0287 - accuracy: 0.9974 - val_loss: 0.9303 - val_accuracy: 0.7431\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_label, batch_size=256, epochs=70, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 7s 20ms/step\n",
      "최종 정확도 : 73.88888597488403 %\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data, test_label, verbose=1)\n",
    "print(\"최종 정확도 : \" + str( score[1] * 100 ) + \" %\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 7s 20ms/step\n",
      "최종 정확도 : 78.88888716697693 %\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data, test_label, verbose=1)\n",
    "print(\"최종 정확도 : \" + str( score[1] * 100 ) + \" %\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2e425b50>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a62db65d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU5d338c8v+0YgkLAlQMK+CSiLCiquVVzQ3taKdtG61d7VLvZpq3d7e1fvp0/31tpSrW3d625VtFSsiqCIShBEwhogkBAgCdn37Xr+mCEmkJABZjKZyff9evEyc+Zk5ndy4jfXXOe6rmPOOUREJPRFBLsAERHxDwW6iEiYUKCLiIQJBbqISJhQoIuIhImoYL1xamqqy8zMDNbbi4iEpLVr15Y459I6ey5ogZ6ZmUl2dnaw3l5EJCSZ2e6unlOXi4hImFCgi4iECQW6iEiYUKCLiIQJBbqISJhQoIuIhAmfAt3MLjKzrWaWa2Z3dvL8SDNbbmbrzGyDmV3s/1JFRORoug10M4sEFgMLgMnANWY2+bDdfgw855w7GVgE/MnfhYr/lVQ38Mr6vcEuQ0T8xJcW+hwg1zm30znXCDwDXH7YPg5I9n7dHyj0X4kSKPe/tZ1vP7Oeosr6YJciIn7gS6CnA/ntHhd4t7X3E+DLZlYALAVu7+yFzOwWM8s2s+zi4uLjKFf8pbXV8a+N+wHILa4OcjUi4g++BLp1su3w2xxdAzzqnMsALgaeMLMjXts595BzbpZzblZaWqdLEUgPyd5dRnFVAwA7i2uCXI2I+IMva7kUACPaPc7gyC6VG4GLAJxzq80sDkgFivxRpPjf0k/3ERMVQYTBDrXQRcKCLy30NcA4M8sysxg8Fz2XHLbPHuA8ADObBMQB6lPppTzdLfs4e3waY9KS2NFDLfT6phbqGlt65L1E+qJuA9051wzcBiwDNuMZzZJjZvea2ULvbt8DbjazT4Cngeud7j7da328p4wDlQ1cMm0YY9KS2NkDLfTy2kY+97uV3PKEVtgUCRSfls91zi3Fc7Gz/ba72329CZjn39IkUP7p7W45d+JgdpXU8OqGQuqbWoiLjgzI+7W2Or777Hr2lNayp7SWXSU1ZKUmBuS9Dqmoa+Lmx7L5zgXjmDsmNaDvJdJbaKZoH9Pa6vjXp/s5a1wa/eKiGZOWhHOwqyRw3S6Ll+eyfGsx3zp3LJERxnPZ+R2eX59fzuOr86iobfLbez75wW4+yivlxy9tpLG51W+vK9KbKdD7mHX55eyvrOeSaUMBGJOWBATuwui724v57Zvb+PzJ6Xz3gvGcPT6NF9cW0NziCdm6xha+8eRa7n4lh9N+9hY/eunTE66lvqmFR1btIiMlnp0lNTy+Ou/ED0QkBCjQ+5hlOfuJjjTOmzQEoK3rIxBDFwvL6/j2M+sZP7gfP/38VMyML84eQVFVA+9s9Vwzf3DFDvZV1POLK0/isunDeH5tAQt+/y6vfnL8c9NeWFtASXUjv/rCdM6ZkMbv39zeNkRTJJwp0PuYt7cUcdroQSTHRQMQHxNJ+oB4n1rFa3eX8fk/rWJ9fnmH7Q3NLVz38Ef86Z1cDl0Lb2xu5T///jGNza088OVTSIjxXK45d+JgUpNieS47n4KyWh5csYPLpg/n6tkj+eUXprPqh+cyI2MAtz+9jsXLP3u99jbureDi37/LfW9uOyKoW1odf3l3J9NHDOC00QP58aWTqWtq4TdvbD2un5dIKFGg9yH5pbXkFlVz9oTBHbaPTkvstoX+zw37uOYvH7BuT/kR4fjK+kJWbCvml69v5fsvbKCxuZWf/nMT6/PL+fVV0xjt7dYBiI6M4MpT0nl7SxF3/eNTzODOBRPbnk/rF8sTN83h8hnD+dWyrfzo5Y1HhPrf3tvFtgNV3Pfmdub9/G3ueHY9nxZUAPD6xv3sPljLrWeNxswYk5bE9XMzeTY7n7W7y47r5yYSKhTofcg7Wz3zvM6Z0HGW7qGhi521hp1zPPDODr751MdMS+/PLWeN5t3tJWzc6wnQ1lbHn1fsYNKwZL5z/jheWFvApX94l8dW7+bmM7O4aOqwI17zqlkjaG51vLu9hFvnjyF9QHyH52OjIrnv6hnceEYWT324h492lbY9V1HXxNJP97Fozgje/t58rj11JMty9nPZH9/jygfe5zf/3kpWaiKfmzK07Xu+df44hveP56t/+5DlW7ue61ZYXkd5baMPP0n/219RT2urRvrKiVGg9yHLtxYzalDCEUMGx6QlUtPYwoHKI/uZX9uwj1+8voXLpg/nyZtO5ZvnjCUpNoqHVu4E4M3NB9hRXMOt80fznfPH85urprOrpIY5mQP5wUUTj3g9gLGDk5iTNZD0AfF8/awxne5jZnz/wgkMSozhwRU72rYv+aSQhuZWrp41ktFpSfxk4RRW/9d53H3pZEqqG9jprSUy4rMVK5LjovnHf84lMzWRmx7L5u8ffnbTdOccq3JLuOmxbOb94m0uvG8lOYUVvv9Q/WDj3grm/eJtfvDihk7/qIr4yqdx6BIafr1sK2bwvc9NOOK5+qYW3t9RwqLZIzHruDxP+5EuQ/vHtW2va2zhZ0s3M3lYMvddPYPICCMuOpJrTx3JX9/dyfcvnMCDK3aQkRLPJSd5WuJXzsxgTtZAUpNiiY7sur3w0Fdm0tTiiI/peux7XHQk183N5Lf/3saW/ZVMHJrMc2vymTQsmanpyW37JcdFc8MZWVw3N5PtRVVMGNLviNcakhzHc18/ndufXsePXtrIb97YhgFNLa1U1jczMDGGm88czaufFPLFB1fzxy+dwjmHdU0d4pwje3cZj67Ko7SmkcdvnHPUYz0a5xz3vJqDc44X1hYwa1QKi+aMBDyfGL777HrOnjCYb5zd+R8+Ca4nPtjNo6t2sXB6OteeOpK0frFBrUeBHkaeX5tP//joTgN99c6D1De1cs7EI0NqdLtAnzf2s0k4f165g8KKeu5bdHKHFu8N87J4ZNUuvvPsej7eU849C6cQ1S7QRgxM6LbWAQkxPh3TV08fxYMrdvDQip3cdOZoPt1bwf9cNvmIP0oAkRHGxKHJnbyKR2JsFA99ZSaPrMpjd+ln1wymZQxg4fThxEVHcuMZWdzw6BpueiybexZO4cunjerwGqtyS/jZvzazcW8lCTGR1Da2sGR9IVfOzPDpeA736oZ9rMkr46efn8rrG/dz95Icpgzvjxnc8Ogaiqoa+HBXKeOHJLWNTJLga2l1/GzpZv763i5GDkzgd29uY/HyXC6YPISUxOhuv3/h9HTmZA30e10K9DBRUt3AgcqGLtdKeWdLEXHREZzayS/RkORYEmMiO1wY3Vtex4MrdnDptGFH/OIN7R/HFTPSeX5tASkJ0Vw16/jCzBcDEmJYNHskj6/Oo6axmZjICK6Ycfjqzb6Liozg5rNGd/n8oZb8t55ex49f3sie0lruvGgiERHGUx/u4b9f2cjIgQn8v8+fxOUzhnPlA+/z55U7+I9T0jv9I9PY3ErewRrGd/Kp4dAnoCnDk1k0eyQLpg7j0vvf5ebHs6msb2JAfDRLbpvHf730Kd99dj2v3X4mIwcd/Y9lfVMLVfXNR7QU65taWLenvK1LJykuipPS+3daM3iujawvKKfe+/sUEWFMzxhwxCeq3KIqhvWPJzG2Y5QcqKwnJjKClETf/nBX1jd5rst4e5xS+8V2+jPrqtYNeyuobWg++o7m+eOdFOtb7FXUNVFa03hEF2VVfRP/5/lPWJZzgOvnZvLfl04m72ANj7+fx+s5+2lu6b7bbHrGAAW6dC2nsBKAyvpm6hpbOvyP55xj+dZi5o1J7XR6v5kxOi2pw9DFny3dDMBdF0/q9P2+Pn80/1i3lxvmZbUNSQyUG8/M4vHVeSzLOcCl04b5HBLHKzE2ij9/ZSb3vraJh1buJL+0lpEDE/jzyp2cMyGNP1x7SlsofH3+aL777Ccs31rEuRM/a0GXVDfw1Id7ePKD3RRVNfDUzacesQTBoTH4v/d+AhqYGMOfvjyTLz64mglD+/G362YxODmOB740k0vuf5dv/H0tL35jbpdLNNQ3tfCFB9+nuKqBd39wLjFRn31quv+t7fzpnR0d9h83OInr52Xy+ZPT285hZX0Tz2cX8PjqPHYfrO2w/4CEaBbNHsm1c0bySUE5j6zaxcd7yhk7OIlHrp/d9sls5bZi/vPvHxMXHcnD189iWsaALn/WO4qreez9PF5YW0DtYY2R710wntvOHdvlH52ahmZe/LiAR9/P83keReagBB752pxul57ILari+kfWUFBWx+mjB/G1eZmMHZzEkx/s4fnsfKobm/mfyybztXlZgKfb8p7Lp3LP5VN9qiNQLFgXYWbNmuWys7VQk78sXp7Lr5Z5hhMu/z9nd/iF3VFczXm/WcH/XjGVrxzWhXDId55Zx5q8Mh792mweXpXH0x/t4Tvnj+M754/v8j33HKwlPSW+Q3dMoNzx7Hr+sW4vT9w4hzPH9cxa+s45Hl6Vx//95yacgy+fNpKfXNaxe6mppZWzf/UO6QPiee7W0wFPv+r/vraJxuZWzhqfxvo9ZcyfMJg/XHNy2/cdqKznrF8u54LJQ/jjtad0eN/C8joGJcUQG/VZcL+1+QA3PpZNQkwkUd6f97yxqfzqqultf1zu+scGnv7Is6zCA186hQXe6xpNLa2c/rO3mTi0H7efOxaA3QdreWx1HjmFlcRERRDnDf/6plYaW1qZOSqFL582kuH9PSOQqhuaeT67gDc27efQYJxRgxK4fPpwHn0/j5ioCP563Ww276vkxy9vZNzgJKobmimpbuD+RSd3GHXU2upYub2YR1blsWJbMTGREVw2fTgLZwxvq+Ppj/bw8vpCrpqZwU8/fxJ7Smt49P08Xt+4v20ph0O1Ts/oz1dOz2RESsfRUoc7WNPIj1/eSKtz/OWrs5iW0Z+ln+7jyQ/2EGHw5dNGsWDqMLJ3l3LrE2uJiYrkmjkjeHFtAYUVnrt6RUUYl0wbxo1nZB31D1Ugmdla59yszp5TCz1MbPK20MEzBK59oC/f4hmqd/b4roNwdFoSL68v5ILfrSQ2KoJr5ozk1vlHvxDX3cd/f/rhgolMHp7MvB5caMvMuPGMLMYPSaKosqHTbpXoyAhuPCOLe1/bRHZeKa9v3M9f39vFORPS+NElkxg7uB//88pGnl6TT3ltY9u1g4ff20VTSys/uPDIkUDDBxwZTOdNGsIfrz2Z7DzPWPr6phaeX1tA3oOrefj6Wby3vYSnP8rn6/NH8/K6vTybnd8W6Mu3FFFS3cB1c0/i1NGDADh19CCumpXB2t1lLMvZT5O3myA2KoJLpg3rNKzOmzSEgrJaXv1kH+OHJHHOhMFERBgLZ6TztUc/4qoH36epxTF/fBqLv3QKdY0t3PTYGr7+5FoumzachJhInIM1u0vZWVxDWr9Y7rhgPNeeOpLUpI5dRHOyBjJyUCL3v7WdVbklFFbUExMVwYVThjLI+wktJiqCi6YO5ZSRKT6f0ynDk/naI2v40l8+JDk+mpLqBsakJdLq4NvPrOen/TZTVttI5qBEHvZ+6vj2eeP496YD5JfVcvmMdIYkx3X/RkGiFnoIKiirJTk+um22J8D8Xy0nNiqCbQeque/qGVxx8mf9zLc/vY51e8p474fndvmanxZUcNdLG1gwdRjXzBnJwAB3a4ST2sZm5v78bZqaW6lpbGnrVz30ySWnsIJL7n+PexZO4bq5mVTUNTHv529zzsSOrfZjtWJbMd/8+8ckxkZSXtvEzFEpPH7DHH735jYeeGcHq+48l2H947npsWw+KShn9Z3ndvh04U8l1Q1877lPGJOWxH9dPLHtfeoaW/jRS5+yakdJ274ZKQl89XRPa7h9t1BnXlxbwF/e3cml0zy/l4OSTnwUSVlNI997/hMArp+byRnegQArthfz2Pt5REdG8OurptM/vvuLm8GgFnoYaWpp5YrFqzhjbCr3LfKEQWV9E7sP1vKNs8ew7UA1+w+76XN+aS2jumlNn5TRn9duPzNgdYezhJgobpiXxe/e3Mbdl07mhjOyOjw/ZXh/pqYn8+yafK6bm8nfP9xNdUMzXz/KxVlfzB+fxvO3ns4Nj64hJSGG+685majICL44awSLl+/gxbUFfHHWCJZvLeLmM0cHLMwBUpNieeyGOUdsj4+J5LdXzzju171yZsZxjyDqSkpiDA9fP/uI7edMGNzlUNVQoUAPMe/vOEhJdSNvbDrQtob5Zm93y5ysgTyxejf7KzoGekFZLedryFtA3XbOWK6ePaLLj+NXzxrBf7+Sw9rdZTyyKo8zx6UyNb3/Cb/vpGHJ/PuO+TS3tLZ154walMhpowfyXHYBERFGS6vjiwEciSS9h2aKhpilG/YBUNvY0rZi4UZvoE8ZnsyQ5FgOtGuh1zY2U1Ld6NPYcDl+ERF21L7VhTPSiY2K4PanPqa4qqHLGbLHIyk26ohx/VfPHsGe0loWv53LnMyBHdbTkfClQO8FNhSU+3SvzaaWVpZt2s+l04YxMDGGpZ96wj2nsILB/WIZ3C+OIclxHbpcCsrqAMjoZgSABFb/+GgWTB1KYUU9U9OTmTd2UEDfb8HUYfSLi6KmsYUvzh7R/TdIWFCgB9nyLUUs/OMqTv/5W/z8X1soLK/rct8Pdh6kvLaJhdOHc+GUIby12dPtkrO3kinDPTMkhybHcaBdl0t+qWcssVrowXftqZ4ho7ed0/XYan+Ji47kCzMzSEmI5uKThnb/DRIWFOhB9sA7OxjWP47Tsgbx0ModnPnL5byRs7/TfZd+uo/EmEjOGp/GgqnDqGls4Y1NB8gtrm7rjx3SP46iqoa2lfvaAj1FgR5sc7IG8sFd53W6AmUg3LVgEm/eMT/gE7+k91CgB9Ha3WV8lFfKzWeO5sGvzGTlD85h3OAk7nl10xFdMM0trSzLOcB5k4YQFx3J6WMGMSAhmvvf2k5Lq+vQQm9udRys8SwDm19WR3x0JKlJGobYG7Rf/CzQYqIi/DLMT0KHAj2I/rxih2c69RxPH2dGSgL3LJzC3vI6/ryy4zTtD3aWUlrTyMXeySLRkRFcOHkouUWe6fpThntb6N4Lc4cujOaX1pKREh/wj/giEnwK9CDJLarm35sP8NXTRnX4SHzq6EFcMm0YD67Ywd52/elLN+4jISaSs9vdnOLiaZ5w7x8f3XbR81AL8NDQxfyyOvWfi/QRCvQg+cvKncRERnDd3MwjnrtrwUScg5//awsHKuv57RtbeXndXs6dOLjDwkxzxwyif3w0U4Ynt7XAh3pb6Psr63HOUVBa2+0aFyISHnS1JAiKKut5ad1erp49otM+zoyUBG6dP4bfv7Wdf326jxbnOG/iYH542B2AoiMjePDLM0mO/+w0pibFEGGeLpeKuiaqGprVQhfpIxToQfBebgmNLa1c470zTWdunT+GjXsrGDUokevmjmLUoM6X+zx9TMfxzFGREaT1i2V/RT35pYfGoCvQRfoCBXoQbNxbSVx0BOOHdD17Lz4mkr91st6EL4Z6Jxfllx0ag64uF5G+QH3oQZBTWMHEockBWyxpSHIcByrrNalIpI9RoPew1lbHpsLKDjc59reh/eM8XS5ltfQ/bJldEQlfPgW6mV1kZlvNLNfM7uzk+d+Z2Xrvv21mVu7/UsNDflktVQ3NbePGA2FIchyV9c1sO1Ct7haRPqTbPnQziwQWAxcABcAaM1vinNt0aB/n3Hfb7X87cPyr9oe5nHYrIwbKoaGLn+SXc+7E0F7fWUR850sLfQ6Q65zb6ZxrBJ4BLj/K/tcAT/ujuHC0cW8FURHm8x3Nj8ehyUUNza3qPxfpQ3wJ9HQgv93jAu+2I5jZKCALeLuL528xs2wzyy4uLj7WWsNCTmElYwcndXnndn9ovy63JhWJ9B2+BHpni4B0dSPSRcALzrlOF/d2zj3knJvlnJuVltYzd27vTZxz5BRW+OVONUfTfgGoDLXQRfoMXwK9AGi/Qn4GUNjFvotQd0uXiqoaKKluDGj/OXjuYJMU67k8omVzRfoOXwJ9DTDOzLLMLAZPaC85fCczmwCkAKv9W2L4yCmsAAh4Cx1gSLJnSQHdqUik7+g20J1zzcBtwDJgM/Cccy7HzO41s4Xtdr0GeMY511V3TJ+3cW8lZp4b+wba0P5xDO4XG9C+ehHpXXya+u+cWwosPWzb3Yc9/on/ygpPOYUVZA5KbOsOCaSrZ49kf0XXt7MTkfCjtVx60Ma9lZw8ckCPvNfC6cN75H1EpPfQ1P8eUl7byN7yuoDOEBWRvk2B3kM2eWeIBnINFxHp2xToPWTL/ioAJg5VoItIYCjQe8j2oipSEqJJTYoJdikiEqYU6D1k+4Fqxg3u13bvTxERf1Og9wDnHNuLqhl3lDsUiYicKAV6DyiuaqCirolxgxXoIhI4CvQesL2oGiCgS+aKiCjQe8C2A54RLmPV5SIiAaRA7wHbi6rpHx9NWlJssEsRkTCmQO8BuQeqGT8kSSNcRCSgFOgB5pxjW1EVYwer/1xEAkuBHmAl1Y2U1zYxXv3nIhJgCvQA217kuSA6Ti10EQkwBXqAbT/gGbKoSUUiEmgK9ADbXlRFclwUg/tphIuIBJYCPcC2Hahm3BCt4SIigadAD7DcompN+ReRHqFAD6CD1Q2U1jQyTlP+RaQHKNADaNuhC6JqoYtID1CgB9CW/Z7bzmlRLhHpCQr0APpg50EyUuIZ2j8u2KWISB+gQA+Q1lbHBztLmTtmULBLEZE+QoEeIJv2VVJR18TcManBLkVE+ggFeoCs3nEQgNPVQheRHqJAD5D3d5QwJi2RIcnqPxeRnqFAD4CmllY+2lWq7hYR6VEK9ADYUFBBTWOLLoiKSI/yKdDN7CIz22pmuWZ2Zxf7fNHMNplZjpk95d8yQ8vqHSUAnDZagS4iPSequx3MLBJYDFwAFABrzGyJc25Tu33GAXcB85xzZWY2OFAFh4L3dxxk8rBkUhJjgl2KiPQhvrTQ5wC5zrmdzrlG4Bng8sP2uRlY7JwrA3DOFfm3zNBR39RC9u4ydbeISI/zJdDTgfx2jwu829obD4w3s1Vm9oGZXdTZC5nZLWaWbWbZxcXFx1dxL/fxnjIam1uZO1aBLiI9y5dA72whb3fY4yhgHHA2cA3wVzMbcMQ3OfeQc26Wc25WWlrasdYaErLzyjCD2ZkDg12KiPQxvgR6ATCi3eMMoLCTfV5xzjU553YBW/EEfJ+z+2AtQ5Pj6BcXHexSRKSP8SXQ1wDjzCzLzGKARcCSw/Z5GTgHwMxS8XTB7PRnoaEiv6yWESkJwS5DRPqgbgPdOdcM3AYsAzYDzznncszsXjNb6N1tGXDQzDYBy4HvO+cOBqro3qygtJaMgfHBLkNE+qBuhy0COOeWAksP23Z3u68dcIf3X5/V2NzKvsp6tdBFJCg0U9SPCsvrcA4yUtRCF5Gep0D3o/yyWgBGDFQLXUR6ngLdj/JL6wAFuogEhwLdj/LLaomONIZqyVwRCQIFuh/ll9YyfEA8kRGdzcUSEQksBbof5ZfVaYSLiASNAt2PCkprGaEx6CISJAp0P6lpaOZgTSMZaqGLSJAo0P2koEwjXEQkuBTofpJf6h2DrklFIhIkCnQ/0aQiEQk2Bbqf5JfWER8dySDddk5EgkSB7if5ZZ4RLmYagy4iwaFA95P8Uq2DLiLBpUD3A+ccBWV16j8XkaBSoPtBRV0T1Q3NWjZXRIJKge4Hh1ZZ1KQiEQkmBboffDZkUS10EQkeBboftE0qUh+6iASRAt0P1uSVktYvluS46GCXIiJ9mAL9BG0/UMWbm4u4ds7IYJciIn2cAv0EPbRyJ3HREVw3NzPYpYhIH6dAPwH7Kup4ef1erp41goGa8i8iQaZAPwEPv7eLVgc3nTk62KWIiCjQj1dFbRNPfbiHS04aptEtItIrKNCP05Mf7qamsYWvz1frXER6BwX6cahvauGRVXmcOS6VKcP7B7scERFAgX5cXvy4gJLqBr4xf0ywSxERaaNAP0YtrY6/rNzJtIz+nD5mULDLERFpo0A/Rsty9pN3sJZb54/RzSxEpFfxKdDN7CIz22pmuWZ2ZyfPX29mxWa23vvvJv+XGnzOOR5csYPMQQlcOGVosMsREekgqrsdzCwSWAxcABQAa8xsiXNu02G7Puucuy0ANfYaq3ceZENBBT/9/FQiI9Q6F5HexZcW+hwg1zm30znXCDwDXB7YsnqnJz/YTWpSDFeekhHsUkREjuBLoKcD+e0eF3i3He5KM9tgZi+Y2YjOXsjMbjGzbDPLLi4uPo5yg2vLvipmZw4kLjoy2KWIiBzBl0DvrG/BHfb4VSDTOTcNeBN4rLMXcs495Jyb5ZyblZaWdmyVBllzSyt7SmvJTE0MdikiIp3yJdALgPYt7gygsP0OzrmDzrkG78O/ADP9U17vsbe8juZWR9YgBbqI9E6+BPoaYJyZZZlZDLAIWNJ+BzMb1u7hQmCz/0rsHXaV1ACohS4ivVa3o1ycc81mdhuwDIgEHnbO5ZjZvUC2c24J8C0zWwg0A6XA9QGsOSjy2gJdC3GJSO/UbaADOOeWAksP23Z3u6/vAu7yb2m9S97BWhJjIklLig12KSIindJMUR/tKqkhMzVRs0NFpNdSoPso72ANWeo/F5FeTIHug6aWVgrK6hToItKrKdB9kF9aS0urI1NDFkWkF1Og+yDvoIYsikjvp0D3wa6SWgB1uYhIr6ZA98GukmqS46JISYgOdikiIl1SoPsgr6SWLA1ZFJFeToHug0Nj0EVEejMFejfqm1oorKjTCBcR6fUU6N3IL63FOV0QFZHeT4HeDa2yKCKhQoHejUNj0LUOuoj0dgr0buwqqSUlIZr+GrIoIr2cAr0buw9qhIuIhAYFejf2V9STPiA+2GWIiHRLgd6N4qoGUnVTCxEJAQr0o6hvaqGqoZm0fgp0Een9FOhHUVzVAKBAF5GQoEA/iuJqBbqIhA4F+lG0tdDVhy4iIUCBfhTqchGRUKJAP4qS6gbMYGBiTLBLERHplgL9KIqrGkhJiCE6Uj8mEen9lFRHUVzVoP5zEQkZCvSjKMdomsIAAAo7SURBVK5uUP+5iIQMBfpRlCjQRSSEKNC74JzzTvvXBVERCQ0K9C5UNzRT39SqFrqIhAyfAt3MLjKzrWaWa2Z3HmW/L5iZM7NZ/isxOEqqGwGNQReR0NFtoJtZJLAYWABMBq4xs8md7NcP+Bbwob+LDIZDk4q00qKIhApfWuhzgFzn3E7nXCPwDHB5J/v9L/BLoN6P9QWNZomKSKjxJdDTgfx2jwu829qY2cnACOfca0d7ITO7xcyyzSy7uLj4mIvtScVVnr9LGocuIqHCl0C3Tra5tifNIoDfAd/r7oWccw8552Y552alpaX5XmUQlFQ3EhlhpCRolIuIhAZfAr0AGNHucQZQ2O5xP2Aq8I6Z5QGnAUtC/cJocVUDgxJjiIjo7O+ZiEjv40ugrwHGmVmWmcUAi4Alh550zlU451Kdc5nOuUzgA2Chcy47IBX3EM0SFZFQ022gO+eagduAZcBm4DnnXI6Z3WtmCwNdYLAUVynQRSS0RPmyk3NuKbD0sG13d7Hv2SdeVvCVVDcwYWi/YJchIuIzzRTtRGur0zouIhJyFOidqKhroqnFaciiiIQUBXonSnRzaBEJQQr0Tmjav4iEIgV6J4rVQheREKRA74TWcRGRUKRA70RxdQMxkREkx/k0qlNEpFdQoHfi0KQiM037F5HQoSYontvNXfbH94g047q5mRSW15Gq7hYRCTEKdCCnsJKNeytJSYjmjuc+AeD8SUOCXJWIyLFRoAPvbC0C4I3vzmfzvkqeWbOHC6cMDXJVIiLHRoEOvL2liOkZ/UnrF0tavzTOGt+712oXEelMn78oWlrTyLr8cs6eMDjYpYiInJA+H+jvbi/GOThnogJdREJbnw/05VuKGJQYw7T0/sEuRUTkhPTpQG9pdazYVsz88Wm61ZyIhLw+HeifFJRTVtvE2epuEZEw0KcDffmWIiIMzhqXGuxSREROWN8O9K1FzByVwoCEmGCXIiJywkI+0J1zx/V9r20oZOPeSj43WROIRCQ8hHSgr9xWzEk/eYNX1u89pu/LLarmhy9sYOaoFK6bmxmY4kREelhIB/q724upbmjm28+s5w9vbfeptV7T0Mw3nlxLXHQki689hZiokP4RiIi0Cemp/1v2VzFxaD8mDUvmN//exu7SWv7r4kkMTPysT7y8tpE3cg5Q29gMwMrtJeworuaJG09laP+4YJUuIuJ3IR/oZ41L49dXTWPEwATuf2s7r35SyBUz0rl42jBe37ifl9YVUN/U2vY9ZnDXgonMG6uRLSISXkI20A9WN1Bc1cCkYf0wM+64YDyXThvGI6vyeGldAc9m5xMbFcEVM9L5yumjSB8QD0BkpJEcFx3k6kVE/C9kA33r/ioAJg5Nbts2fkg/fvYfJ/HDiyawesdBTh09qEP3i4hIOAvZQN/sDfQJQ/sd8dyAhBgWnDSsp0sSEQmqkB3isXV/JalJMaTpVnEiIkAIB7pnhEty9zuKiPQRPgW6mV1kZlvNLNfM7uzk+VvN7FMzW29m75nZZP+X+pmWVsfW/VWddreIiPRV3Qa6mUUCi4EFwGTgmk4C+ynn3EnOuRnAL4Hf+r3SdnYfrKGhuZWJCnQRkTa+tNDnALnOuZ3OuUbgGeDy9js45yrbPUwEjm+BFR9t8V4QnTRMXS4iIof4MsolHchv97gAOPXwnczsm8AdQAxwbmcvZGa3ALcAjBw58lhrbbNlXyURBmMHJx33a4iIhBtfWuid3crniBa4c26xc24M8EPgx529kHPuIefcLOfcrLS0tGOrtJ0t+6vISk0kLjryuF9DRCTc+BLoBcCIdo8zgMKj7P8McMWJFNWdLfurmKjuFhGRDnwJ9DXAODPLMrMYYBGwpP0OZjau3cNLgO3+K7Gj6oZm9pTWMnGILoiKiLTXbR+6c67ZzG4DlgGRwMPOuRwzuxfIds4tAW4zs/OBJqAMuC5QBW874J3yrxa6iEgHPk39d84tBZYetu3udl9/2891dWnLvkNruKiFLiLSXsjNFE1NiuFzk4eQkRIf7FJERHqVkFuc63NThvK5KboPqIjI4UKuhS4iIp1ToIuIhAkFuohImFCgi4iECQW6iEiYUKCLiIQJBbqISJhQoIuIhAlzLqD3ouj6jc2Kgd3H+e2pQIkfywkVffG4++IxQ9887r54zHDsxz3KOdfp+uNBC/QTYWbZzrlZwa6jp/XF4+6Lxwx987j74jGDf49bXS4iImFCgS4iEiZCNdAfCnYBQdIXj7svHjP0zePui8cMfjzukOxDFxGRI4VqC11ERA6jQBcRCRMhF+hmdpGZbTWzXDO7M9j1BIKZjTCz5Wa22cxyzOzb3u0DzezfZrbd+9+UYNfqb2YWaWbrzOw17+MsM/vQe8zPem9UHlbMbICZvWBmW7zn/PQ+cq6/6/393mhmT5tZXLidbzN72MyKzGxju22dnlvzuN+bbRvM7JRjfb+QCnQziwQWAwuAycA1ZjY5uFUFRDPwPefcJOA04Jve47wTeMs5Nw54y/s43Hwb2Nzu8S+A33mPuQy4MShVBdbvgdedcxOB6XiOP6zPtZmlA98CZjnnpuK5Af0iwu98PwpcdNi2rs7tAmCc998twAPH+mYhFejAHCDXObfTOdcIPANcHuSa/M45t88597H36yo8/4On4znWx7y7PQZcEZwKA8PMMoBLgL96HxtwLvCCd5dwPOZk4CzgbwDOuUbnXDlhfq69ooB4M4sCEoB9hNn5ds6tBEoP29zVub0ceNx5fAAMMLNhx/J+oRbo6UB+u8cF3m1hy8wygZOBD4Ehzrl94Al9YHDwKguI+4AfAK3ex4OAcudcs/dxOJ7v0UAx8Ii3q+mvZpZImJ9r59xe4NfAHjxBXgGsJfzPN3R9bk8430It0K2TbWE77tLMkoAXge845yqDXU8gmdmlQJFzbm37zZ3sGm7nOwo4BXjAOXcyUEOYda90xttvfDmQBQwHEvF0ORwu3M730Zzw73uoBXoBMKLd4wygMEi1BJSZReMJ87875/7h3Xzg0Ecw73+LglVfAMwDFppZHp6utHPxtNgHeD+SQ3ie7wKgwDn3offxC3gCPpzPNcD5wC7nXLFzrgn4BzCX8D/f0PW5PeF8C7VAXwOM814Jj8FzEWVJkGvyO2/f8d+Azc6537Z7aglwnffr64BXerq2QHHO3eWcy3DOZeI5r287574ELAe+4N0trI4ZwDm3H8g3swneTecBmwjjc+21BzjNzBK8v++Hjjusz7dXV+d2CfBV72iX04CKQ10zPnPOhdQ/4GJgG7AD+FGw6wnQMZ6B56PWBmC999/FePqU3wK2e/87MNi1Buj4zwZe8349GvgIyAWeB2KDXV8AjncGkO093y8DKX3hXAP3AFuAjcATQGy4nW/gaTzXCJrwtMBv7Orc4ulyWezNtk/xjAA6pvfT1H8RkTARal0uIiLSBQW6iEiYUKCLiIQJBbqISJhQoIuIhAkFuohImFCgi4iEif8PLfTZvZUyQIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 16.9020 - accuracy: 0.3116\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 1.4089 - accuracy: 0.3776\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.2639 - accuracy: 0.3646\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.1093 - accuracy: 0.4670\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.8712 - accuracy: 0.6285\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.6662 - accuracy: 0.7118\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.6245 - accuracy: 0.6979\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.5302 - accuracy: 0.7526\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.4407 - accuracy: 0.8021\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.4140 - accuracy: 0.7899\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.3782 - accuracy: 0.8394\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.3632 - accuracy: 0.8342\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.3418 - accuracy: 0.8464\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.3142 - accuracy: 0.8681\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.3089 - accuracy: 0.8715\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.2419 - accuracy: 0.9002\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.2667 - accuracy: 0.8837\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.2446 - accuracy: 0.9028\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2295 - accuracy: 0.9071\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.1811 - accuracy: 0.9323\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1591 - accuracy: 0.9358\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.1950 - accuracy: 0.9158\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.1626 - accuracy: 0.9401\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1518 - accuracy: 0.9392\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.1281 - accuracy: 0.9592\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1153 - accuracy: 0.9653\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0897 - accuracy: 0.9696\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.1015 - accuracy: 0.9722\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0844 - accuracy: 0.9774\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0793 - accuracy: 0.9774\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0673 - accuracy: 0.9835\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0866 - accuracy: 0.9696\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0695 - accuracy: 0.9800\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0540 - accuracy: 0.9852\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0725 - accuracy: 0.9757\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0670 - accuracy: 0.9766\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0519 - accuracy: 0.9852\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0468 - accuracy: 0.9913\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0516 - accuracy: 0.9878\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0415 - accuracy: 0.9913\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0388 - accuracy: 0.9887\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0408 - accuracy: 0.9861\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0421 - accuracy: 0.9870\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0365 - accuracy: 0.9887\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0296 - accuracy: 0.9922\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0393 - accuracy: 0.9870\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0317 - accuracy: 0.9896\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0307 - accuracy: 0.9931\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.0207 - accuracy: 0.9939\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0227 - accuracy: 0.9965\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 62s 54ms/step - loss: 0.0130 - accuracy: 0.9974\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0297 - accuracy: 0.9931\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0298 - accuracy: 0.9913\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0217 - accuracy: 0.9965\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0201 - accuracy: 0.9948\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0145 - accuracy: 0.9974\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0169 - accuracy: 0.9948\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0320 - accuracy: 0.9905\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0404 - accuracy: 0.9878\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0362 - accuracy: 0.9931\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0360 - accuracy: 0.9887\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0286 - accuracy: 0.9905\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0277 - accuracy: 0.9922\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.0203 - accuracy: 0.9948\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0218 - accuracy: 0.9913\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0213 - accuracy: 0.9957\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0228 - accuracy: 0.9931\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0189 - accuracy: 0.9948\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0555 - accuracy: 0.9835\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0726 - accuracy: 0.9740\n",
      "288/288 [==============================] - 6s 22ms/step\n",
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 13.6956 - accuracy: 0.2769\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 1.5118 - accuracy: 0.3733\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 1.4593 - accuracy: 0.4488\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 1.3228 - accuracy: 0.4957\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 1.0583 - accuracy: 0.5417\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.7187 - accuracy: 0.6780\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.6799 - accuracy: 0.6840\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.5959 - accuracy: 0.7248\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.5065 - accuracy: 0.7587\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.4992 - accuracy: 0.7734\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.4414 - accuracy: 0.7977\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.3761 - accuracy: 0.8385\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.3671 - accuracy: 0.8307\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.3521 - accuracy: 0.8403\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.3416 - accuracy: 0.8559\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.3137 - accuracy: 0.8689\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.2980 - accuracy: 0.8793\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2842 - accuracy: 0.8932\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2688 - accuracy: 0.8811\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2333 - accuracy: 0.9158\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2465 - accuracy: 0.9045\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2162 - accuracy: 0.9158\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.2297 - accuracy: 0.9080\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.1950 - accuracy: 0.9280\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.1787 - accuracy: 0.9375\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.1600 - accuracy: 0.9401\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.1394 - accuracy: 0.9497\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.1642 - accuracy: 0.9392\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.1419 - accuracy: 0.9523\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.1361 - accuracy: 0.9462\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.1368 - accuracy: 0.9514\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.1255 - accuracy: 0.9540\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.1342 - accuracy: 0.9531\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.1528 - accuracy: 0.9418\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.1121 - accuracy: 0.9635\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.1244 - accuracy: 0.9592\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.0845 - accuracy: 0.9740\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.0798 - accuracy: 0.9792\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0789 - accuracy: 0.9774\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0635 - accuracy: 0.9800\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0565 - accuracy: 0.9861\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0688 - accuracy: 0.9783\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0683 - accuracy: 0.9757\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0920 - accuracy: 0.9679\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0966 - accuracy: 0.9661\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0621 - accuracy: 0.9835\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0684 - accuracy: 0.9783\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0578 - accuracy: 0.9826\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0492 - accuracy: 0.9870\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0359 - accuracy: 0.9939\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0353 - accuracy: 0.9905\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0315 - accuracy: 0.9922\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0312 - accuracy: 0.9931\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0293 - accuracy: 0.9931\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.0408 - accuracy: 0.9905\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0425 - accuracy: 0.9852\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0390 - accuracy: 0.9878\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0352 - accuracy: 0.9913\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0222 - accuracy: 0.9957\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0259 - accuracy: 0.9939\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0198 - accuracy: 0.9948\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0329 - accuracy: 0.9896\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0301 - accuracy: 0.9939\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0248 - accuracy: 0.9922\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0358 - accuracy: 0.9939\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0352 - accuracy: 0.9870\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0326 - accuracy: 0.9870\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0360 - accuracy: 0.9896\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0244 - accuracy: 0.9939\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0296 - accuracy: 0.9922\n",
      "288/288 [==============================] - 6s 22ms/step\n",
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 13.6108 - accuracy: 0.3516\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 1.6037 - accuracy: 0.3863\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.5555 - accuracy: 0.3611\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.4794 - accuracy: 0.3828\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.4047 - accuracy: 0.4045\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 1.3094 - accuracy: 0.4566\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.0690 - accuracy: 0.5503\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.8305 - accuracy: 0.6181\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.6716 - accuracy: 0.6814\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.5602 - accuracy: 0.7457\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.4923 - accuracy: 0.7700\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.4606 - accuracy: 0.7856\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.4169 - accuracy: 0.8177\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.3585 - accuracy: 0.8594\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.3272 - accuracy: 0.8611\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.3281 - accuracy: 0.8576\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.3361 - accuracy: 0.8472\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.2902 - accuracy: 0.8698\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.2492 - accuracy: 0.8915\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2286 - accuracy: 0.9071\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.2496 - accuracy: 0.8898\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.2350 - accuracy: 0.9106\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2031 - accuracy: 0.9280\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1942 - accuracy: 0.9236\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.1746 - accuracy: 0.9427\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 66s 58ms/step - loss: 0.1616 - accuracy: 0.9444\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.1307 - accuracy: 0.9575\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.1493 - accuracy: 0.9436\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 59s 52ms/step - loss: 0.1240 - accuracy: 0.9592\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.1323 - accuracy: 0.9523\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.1582 - accuracy: 0.9384\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.1551 - accuracy: 0.9436\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1834 - accuracy: 0.9236\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.1813 - accuracy: 0.9193\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.1338 - accuracy: 0.9540\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.1159 - accuracy: 0.9679\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0914 - accuracy: 0.9679\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0893 - accuracy: 0.9740\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0865 - accuracy: 0.9688\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0674 - accuracy: 0.9826\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0602 - accuracy: 0.9896\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0718 - accuracy: 0.9766\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0484 - accuracy: 0.9887\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0480 - accuracy: 0.9878\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0574 - accuracy: 0.9826\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0743 - accuracy: 0.9826\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0534 - accuracy: 0.9844\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0647 - accuracy: 0.9757\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.0636 - accuracy: 0.9792\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0623 - accuracy: 0.9844\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0454 - accuracy: 0.9905\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0486 - accuracy: 0.9878\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0488 - accuracy: 0.9861\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0403 - accuracy: 0.9887\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.0339 - accuracy: 0.9922\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0339 - accuracy: 0.9922\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0419 - accuracy: 0.9931\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0443 - accuracy: 0.9861\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0291 - accuracy: 0.9939\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0334 - accuracy: 0.9896\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0209 - accuracy: 0.9974\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0238 - accuracy: 0.9939\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0437 - accuracy: 0.9870\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0356 - accuracy: 0.9905\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 54950s 48s/step - loss: 0.0269 - accuracy: 0.9922\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0313 - accuracy: 0.9931\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0296 - accuracy: 0.9887\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.0241 - accuracy: 0.9931\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0226 - accuracy: 0.9939\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.0297 - accuracy: 0.9913\n",
      "288/288 [==============================] - 6s 22ms/step\n",
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 12.9189 - accuracy: 0.3168\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.5774 - accuracy: 0.3533\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.5084 - accuracy: 0.3507\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.3371 - accuracy: 0.3837\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 1.1668 - accuracy: 0.4462\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 55s 48ms/step - loss: 0.9714 - accuracy: 0.5530\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.8972 - accuracy: 0.5877\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.7017 - accuracy: 0.6727\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.6287 - accuracy: 0.7205\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.6056 - accuracy: 0.7040\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.5307 - accuracy: 0.7561\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.4278 - accuracy: 0.7977\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.3704 - accuracy: 0.8429\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.3421 - accuracy: 0.8568\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.3122 - accuracy: 0.8663\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.2933 - accuracy: 0.8594\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.2534 - accuracy: 0.8828\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.2237 - accuracy: 0.9115\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.2066 - accuracy: 0.9271\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.2045 - accuracy: 0.9132\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1874 - accuracy: 0.9280\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 52s 46ms/step - loss: 0.1875 - accuracy: 0.9236\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.2006 - accuracy: 0.9089\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 52s 46ms/step - loss: 0.1579 - accuracy: 0.9427\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1477 - accuracy: 0.9523\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1508 - accuracy: 0.9462\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1098 - accuracy: 0.9653\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0861 - accuracy: 0.9714\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0732 - accuracy: 0.9818\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0572 - accuracy: 0.9878\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.0654 - accuracy: 0.9809\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0527 - accuracy: 0.9896\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0613 - accuracy: 0.9826\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0445 - accuracy: 0.9931\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0343 - accuracy: 0.9931\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0358 - accuracy: 0.9922\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0295 - accuracy: 0.9939\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.0406 - accuracy: 0.9870\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 57s 50ms/step - loss: 0.0444 - accuracy: 0.9870\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0652 - accuracy: 0.9792\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 79s 69ms/step - loss: 0.0624 - accuracy: 0.9844\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 55s 47ms/step - loss: 0.0515 - accuracy: 0.9835\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0626 - accuracy: 0.9818\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0382 - accuracy: 0.9913\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0325 - accuracy: 0.9913\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0322 - accuracy: 0.9896\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0327 - accuracy: 0.9913\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0252 - accuracy: 0.9957\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0182 - accuracy: 0.9974\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0207 - accuracy: 0.9957\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0193 - accuracy: 0.9957\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0161 - accuracy: 0.9983\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0100 - accuracy: 0.9983\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0158 - accuracy: 0.9974\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0134 - accuracy: 0.9965\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0169 - accuracy: 0.9948\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0145 - accuracy: 0.9957\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0095 - accuracy: 0.9983\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0087 - accuracy: 0.9983\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0093 - accuracy: 0.9991\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0179 - accuracy: 0.9974\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0163 - accuracy: 0.9948\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0180 - accuracy: 0.9948\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0116 - accuracy: 0.9983\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0166 - accuracy: 0.9957\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0183 - accuracy: 0.9948\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0126 - accuracy: 0.9983\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0123 - accuracy: 0.9965\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0086 - accuracy: 0.9983\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0100 - accuracy: 0.9983\n",
      "288/288 [==============================] - 6s 21ms/step\n",
      "Epoch 1/70\n",
      "1152/1152 [==============================] - 113s 98ms/step - loss: 9.5444 - accuracy: 0.3759\n",
      "Epoch 2/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.3569 - accuracy: 0.4149\n",
      "Epoch 3/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 1.0525 - accuracy: 0.4957\n",
      "Epoch 4/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.7222 - accuracy: 0.6927\n",
      "Epoch 5/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.6046 - accuracy: 0.7309\n",
      "Epoch 6/70\n",
      "1152/1152 [==============================] - 57s 49ms/step - loss: 0.4780 - accuracy: 0.7778\n",
      "Epoch 7/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.4451 - accuracy: 0.8073\n",
      "Epoch 8/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.3848 - accuracy: 0.8351\n",
      "Epoch 9/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.3552 - accuracy: 0.8464\n",
      "Epoch 10/70\n",
      "1152/1152 [==============================] - 66s 57ms/step - loss: 0.2830 - accuracy: 0.8802\n",
      "Epoch 11/70\n",
      "1152/1152 [==============================] - 56s 48ms/step - loss: 0.2859 - accuracy: 0.8802\n",
      "Epoch 12/70\n",
      "1152/1152 [==============================] - 56s 49ms/step - loss: 0.2873 - accuracy: 0.8932\n",
      "Epoch 13/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.2414 - accuracy: 0.9123\n",
      "Epoch 14/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.1821 - accuracy: 0.9288\n",
      "Epoch 15/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.1797 - accuracy: 0.9253\n",
      "Epoch 16/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.1940 - accuracy: 0.9410\n",
      "Epoch 17/70\n",
      "1152/1152 [==============================] - 62s 54ms/step - loss: 0.1785 - accuracy: 0.9332\n",
      "Epoch 18/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.1477 - accuracy: 0.9575\n",
      "Epoch 19/70\n",
      "1152/1152 [==============================] - 63s 55ms/step - loss: 0.1257 - accuracy: 0.9618\n",
      "Epoch 20/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.1085 - accuracy: 0.9618\n",
      "Epoch 21/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0956 - accuracy: 0.9714\n",
      "Epoch 22/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.1012 - accuracy: 0.9722\n",
      "Epoch 23/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0727 - accuracy: 0.9809\n",
      "Epoch 24/70\n",
      "1152/1152 [==============================] - 62s 54ms/step - loss: 0.0644 - accuracy: 0.9818\n",
      "Epoch 25/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0705 - accuracy: 0.9792\n",
      "Epoch 26/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0611 - accuracy: 0.9861\n",
      "Epoch 27/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0612 - accuracy: 0.9878\n",
      "Epoch 28/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0498 - accuracy: 0.9905\n",
      "Epoch 29/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0498 - accuracy: 0.9844\n",
      "Epoch 30/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0415 - accuracy: 0.9931\n",
      "Epoch 31/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0466 - accuracy: 0.9861\n",
      "Epoch 32/70\n",
      "1152/1152 [==============================] - 59s 52ms/step - loss: 0.0527 - accuracy: 0.9870\n",
      "Epoch 33/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0505 - accuracy: 0.9844\n",
      "Epoch 34/70\n",
      "1152/1152 [==============================] - 59s 52ms/step - loss: 0.0419 - accuracy: 0.9887\n",
      "Epoch 35/70\n",
      "1152/1152 [==============================] - 59s 52ms/step - loss: 0.0429 - accuracy: 0.9896\n",
      "Epoch 36/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0568 - accuracy: 0.9835\n",
      "Epoch 37/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0391 - accuracy: 0.9896\n",
      "Epoch 38/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0328 - accuracy: 0.9922\n",
      "Epoch 39/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0311 - accuracy: 0.9913\n",
      "Epoch 40/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.0359 - accuracy: 0.9896\n",
      "Epoch 41/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.0299 - accuracy: 0.9922\n",
      "Epoch 42/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0352 - accuracy: 0.9896\n",
      "Epoch 43/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0268 - accuracy: 0.9957\n",
      "Epoch 44/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0181 - accuracy: 0.9974\n",
      "Epoch 45/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0129 - accuracy: 0.9991\n",
      "Epoch 46/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0229 - accuracy: 0.9939\n",
      "Epoch 47/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0138 - accuracy: 0.9991\n",
      "Epoch 48/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0204 - accuracy: 0.9957\n",
      "Epoch 49/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0231 - accuracy: 0.9939\n",
      "Epoch 50/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0350 - accuracy: 0.9887\n",
      "Epoch 51/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0350 - accuracy: 0.9913\n",
      "Epoch 52/70\n",
      "1152/1152 [==============================] - 61s 53ms/step - loss: 0.0254 - accuracy: 0.9974\n",
      "Epoch 53/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0164 - accuracy: 0.9983\n",
      "Epoch 54/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0125 - accuracy: 0.9991\n",
      "Epoch 55/70\n",
      "1152/1152 [==============================] - 60s 52ms/step - loss: 0.0128 - accuracy: 0.9974\n",
      "Epoch 56/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0181 - accuracy: 0.9939\n",
      "Epoch 57/70\n",
      "1152/1152 [==============================] - 58s 51ms/step - loss: 0.0122 - accuracy: 0.9983\n",
      "Epoch 58/70\n",
      "1152/1152 [==============================] - 59s 51ms/step - loss: 0.0120 - accuracy: 0.9965\n",
      "Epoch 59/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0104 - accuracy: 0.9991\n",
      "Epoch 60/70\n",
      "1152/1152 [==============================] - 59s 52ms/step - loss: 0.0101 - accuracy: 0.9983\n",
      "Epoch 61/70\n",
      "1152/1152 [==============================] - 58s 50ms/step - loss: 0.0083 - accuracy: 0.9983\n",
      "Epoch 62/70\n",
      "1152/1152 [==============================] - 54s 47ms/step - loss: 0.0106 - accuracy: 0.9983\n",
      "Epoch 63/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0197 - accuracy: 0.9939\n",
      "Epoch 64/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0239 - accuracy: 0.9939\n",
      "Epoch 65/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0458 - accuracy: 0.9826\n",
      "Epoch 66/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1714 - accuracy: 0.9453\n",
      "Epoch 67/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.1054 - accuracy: 0.9644\n",
      "Epoch 68/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0465 - accuracy: 0.9852\n",
      "Epoch 69/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0256 - accuracy: 0.9931\n",
      "Epoch 70/70\n",
      "1152/1152 [==============================] - 53s 46ms/step - loss: 0.0153 - accuracy: 0.9957\n",
      "288/288 [==============================] - 6s 21ms/step\n",
      "\n",
      "K-fold cross validation Accuracy: ['0.7569', '0.8056', '0.7882', '0.7778', '0.7361']\n"
     ]
    }
   ],
   "source": [
    "skf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for t, v in skf.split(train_data, train_label):\n",
    "    model = LeNet.build(input_shape = (120, 150, 1), classes = 6)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(train_data[t], train_label[t], batch_size=128, epochs=70, verbose=1)\n",
    "\n",
    "    k_accuracy = '%.4f' % (model.evaluate(train_data[v], train_label[v])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "\n",
    "# 전체 검증 결과 출력\n",
    "print('\\nK-fold cross validation Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
